{
 "cells": [
  {
   "cell_type": "raw",
   "id": "97bdce63-94fc-4706-8827-e7ba7c879d9f",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import copy\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: PROPER TRAIN-TEST SPLIT (DO THIS FIRST - NO PREPROCESSING YET)\n",
    "# =============================================================================\n",
    "print(\"=== STEP 1: Initial Data Split ===\")\n",
    "print(f\"Original dataset shape: X{X.shape}, y_7{y_7.shape}, y_28{y_28.shape}\")\n",
    "\n",
    "# Split indices first - this is the ONLY thing we do before any preprocessing\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.arange(len(X)), test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_indices)}\")\n",
    "print(f\"Test samples: {len(test_indices)}\")\n",
    "\n",
    "# Split the raw data using indices\n",
    "X_train_raw = X[train_indices]\n",
    "X_test_raw = X[test_indices]\n",
    "y_7_train_raw = y_7[train_indices]\n",
    "y_7_test_raw = y_7[test_indices]\n",
    "y_28_train_raw = y_28[train_indices]\n",
    "y_28_test_raw = y_28[test_indices]\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: FEATURE SCALING (FIT ON TRAINING ONLY)\n",
    "# =============================================================================\n",
    "print(\"\\n=== STEP 2: Feature Scaling ===\")\n",
    "\n",
    "# Fit scaler on training data ONLY\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = feature_scaler.transform(X_test_raw)  # Only transform, never fit on test\n",
    "\n",
    "print(f\"Training features scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test features scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Verify scaling worked correctly\n",
    "print(f\"Training features mean: {X_train_scaled.mean(axis=0)[:3]}... (should be ~0)\")\n",
    "print(f\"Training features std: {X_train_scaled.std(axis=0)[:3]}... (should be ~1)\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: TARGET SCALING - GLOBAL SCALER (FIT ON TRAINING ONLY)\n",
    "# =============================================================================\n",
    "print(\"\\n=== STEP 3: Target Scaling - Global Scaler ===\")\n",
    "\n",
    "# Combine ONLY training targets to fit the global scaler\n",
    "all_train_targets = np.concatenate([y_7_train_raw, y_28_train_raw])\n",
    "print(f\"Combined training targets shape: {all_train_targets.shape}\")\n",
    "print(f\"Training targets range: [{all_train_targets.min():.2f}, {all_train_targets.max():.2f}]\")\n",
    "\n",
    "# Fit global scaler on training data ONLY\n",
    "global_target_scaler = StandardScaler()\n",
    "global_target_scaler.fit(all_train_targets.reshape(-1, 1))\n",
    "\n",
    "# Transform both training and test targets using the same fitted scaler\n",
    "y_7_train_scaled = global_target_scaler.transform(y_7_train_raw.reshape(-1, 1)).flatten()\n",
    "y_28_train_scaled = global_target_scaler.transform(y_28_train_raw.reshape(-1, 1)).flatten()\n",
    "y_7_test_scaled = global_target_scaler.transform(y_7_test_raw.reshape(-1, 1)).flatten()\n",
    "y_28_test_scaled = global_target_scaler.transform(y_28_test_raw.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"Scaled training targets - 7d: mean={y_7_train_scaled.mean():.3f}, std={y_7_train_scaled.std():.3f}\")\n",
    "print(f\"Scaled training targets - 28d: mean={y_28_train_scaled.mean():.3f}, std={y_28_train_scaled.std():.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: DATASET AND MODEL DEFINITIONS\n",
    "# =============================================================================\n",
    "print(\"\\n=== STEP 4: Dataset and Model Setup ===\")\n",
    "\n",
    "class ConcreteDatasetScaled(Dataset):\n",
    "    def __init__(self, features, y_7_scaled, y_28_scaled):\n",
    "        self.z = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(np.vstack((y_7_scaled, y_28_scaled)).T, dtype=torch.float32)\n",
    "        self.t = torch.tensor([7.0, 28.0], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.t, self.y[idx]\n",
    "\n",
    "class AGPModelGP_v2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 128, 64], dropout_rate=0.05):\n",
    "        super(AGPModelGP_v2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dims[2])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], 5)\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc4(x)\n",
    "        \n",
    "        theta1 = self.softplus(out[:, 0])\n",
    "        theta2 = out[:, 1]\n",
    "        l = torch.exp(out[:, 2])\n",
    "        sigma_f = torch.exp(out[:, 3])\n",
    "        sigma_n = torch.exp(out[:, 4])\n",
    "        \n",
    "        return theta1, theta2, l, sigma_f, sigma_n\n",
    "\n",
    "def gp_neg_log_likelihood(theta1, theta2, l, sigma_f, sigma_n, t, y, epsilon=1e-6):\n",
    "    batch_size = y.shape[0]\n",
    "    n_t = t.shape[0]\n",
    "    total_nll = 0.0\n",
    "    log_t = torch.log(t + epsilon)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        m = theta1[i] * log_t + theta2[i]\n",
    "        diff = t.unsqueeze(0) - t.unsqueeze(1)\n",
    "        K = sigma_f[i]**2 * torch.exp(-0.5 * (diff**2) / (l[i]**2))\n",
    "        K = K + sigma_n[i]**2 * torch.eye(n_t, device=K.device)\n",
    "        K = K + epsilon * torch.eye(n_t, device=K.device)\n",
    "        \n",
    "        L_mat = torch.linalg.cholesky(K)\n",
    "        diff_y = (y[i] - m).unsqueeze(1)\n",
    "        alpha = torch.cholesky_solve(diff_y, L_mat)\n",
    "        log_det_K = 2.0 * torch.sum(torch.log(torch.diag(L_mat)))\n",
    "        nll = 0.5 * diff_y.t() @ alpha + 0.5 * log_det_K + 0.5 * n_t * np.log(2 * np.pi)\n",
    "        total_nll += nll.squeeze()\n",
    "    \n",
    "    return total_nll / batch_size\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: CREATE DATASETS AND DATALOADERS\n",
    "# =============================================================================\n",
    "print(\"\\n=== STEP 5: Creating Datasets ===\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ConcreteDatasetScaled(X_train_scaled, y_7_train_scaled, y_28_train_scaled)\n",
    "test_dataset = ConcreteDatasetScaled(X_test_scaled, y_7_test_scaled, y_28_test_scaled)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32  # You can adjust this\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))  # Full batch for test\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: MODEL TRAINING\n",
    "# =============================================================================\n",
    "print(\"\\n=== STEP 6: Model Training ===\")\n",
    "\n",
    "# Set hyperparameters\n",
    "best_dropout = 0.05\n",
    "best_lr = 0.002\n",
    "best_hidden_dims = [128, 128, 64]\n",
    "num_epochs = 200\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Hyperparameters: dropout={best_dropout}, lr={best_lr}, hidden_dims={best_hidden_dims}\")\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = AGPModelGP_v2(input_dim, hidden_dims=best_hidden_dims, dropout_rate=best_dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "train_losses = []\n",
    "times = torch.tensor([7.0, 28.0], dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for z_batch, _, y_batch in train_loader:\n",
    "        z_batch = z_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        theta1, theta2, l, sigma_f, sigma_n = model(z_batch)\n",
    "        loss = gp_neg_log_likelihood(theta1, theta2, l, sigma_f, sigma_n, times, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: MODEL EVALUATION\n",
    "# =============================================================================\n",
    "print(\"\\n=== STEP 7: Model Evaluation ===\")\n",
    "\n",
    "# Monte Carlo Dropout Prediction Functions\n",
    "def predict_at_times(model, z, times, epsilon=1e-6):\n",
    "    \"\"\"Single forward pass prediction\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        theta1, theta2, l, sigma_f, sigma_n = model(z)\n",
    "        log_times = torch.log(times + epsilon)\n",
    "        means = theta1.unsqueeze(1) * log_times.unsqueeze(0) + theta2.unsqueeze(1)\n",
    "    return means\n",
    "\n",
    "def mc_dropout_predict(model, z, times, n_samples=100, epsilon=1e-6):\n",
    "    \"\"\"Monte Carlo Dropout prediction with uncertainty quantification\"\"\"\n",
    "    model.train()  # Keep in training mode to enable dropout\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            theta1, theta2, l, sigma_f, sigma_n = model(z)\n",
    "            log_times = torch.log(times + epsilon)\n",
    "            means = theta1.unsqueeze(1) * log_times.unsqueeze(0) + theta2.unsqueeze(1)\n",
    "            predictions.append(means.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)  # Shape: [n_samples, n_data, n_times]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    std_pred = np.std(predictions, axis=0)\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    ci_lower = np.percentile(predictions, 2.5, axis=0)\n",
    "    ci_upper = np.percentile(predictions, 97.5, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean_pred,\n",
    "        'std': std_pred,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'all_predictions': predictions\n",
    "    }\n",
    "\n",
    "# Make predictions on test set with MC Dropout\n",
    "print(\"Making predictions with Monte Carlo Dropout...\")\n",
    "z_test = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "times_pred = torch.tensor([7.0, 28.0], dtype=torch.float32).to(device)\n",
    "\n",
    "# MC Dropout predictions (100 samples for uncertainty)\n",
    "mc_results = mc_dropout_predict(model, z_test, times_pred, n_samples=100)\n",
    "y_pred_scaled = mc_results['mean']\n",
    "y_pred_std_scaled = mc_results['std']\n",
    "y_pred_ci_lower_scaled = mc_results['ci_lower']\n",
    "y_pred_ci_upper_scaled = mc_results['ci_upper']\n",
    "\n",
    "# Inverse transform predictions and uncertainties to original scale\n",
    "y_pred_7_original = global_target_scaler.inverse_transform(y_pred_scaled[:, 0].reshape(-1, 1)).flatten()\n",
    "y_pred_28_original = global_target_scaler.inverse_transform(y_pred_scaled[:, 1].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Transform uncertainty bounds to original scale\n",
    "y_pred_7_ci_lower = global_target_scaler.inverse_transform(y_pred_ci_lower_scaled[:, 0].reshape(-1, 1)).flatten()\n",
    "y_pred_7_ci_upper = global_target_scaler.inverse_transform(y_pred_ci_upper_scaled[:, 0].reshape(-1, 1)).flatten()\n",
    "y_pred_28_ci_lower = global_target_scaler.inverse_transform(y_pred_ci_lower_scaled[:, 1].reshape(-1, 1)).flatten()\n",
    "y_pred_28_ci_upper = global_target_scaler.inverse_transform(y_pred_ci_upper_scaled[:, 1].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Transform standard deviations to original scale (approximate)\n",
    "y_pred_7_std = global_target_scaler.scale_[0] * y_pred_std_scaled[:, 0]\n",
    "y_pred_28_std = global_target_scaler.scale_[0] * y_pred_std_scaled[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "r2_7 = r2_score(y_7_test_raw, y_pred_7_original)\n",
    "r2_28 = r2_score(y_28_test_raw, y_pred_28_original)\n",
    "mae_7 = mean_absolute_error(y_7_test_raw, y_pred_7_original)\n",
    "mae_28 = mean_absolute_error(y_28_test_raw, y_pred_28_original)\n",
    "rmse_7 = np.sqrt(mean_squared_error(y_7_test_raw, y_pred_7_original))\n",
    "rmse_28 = np.sqrt(mean_squared_error(y_28_test_raw, y_pred_28_original))\n",
    "\n",
    "print(\"=== MODEL PERFORMANCE WITH UNCERTAINTY ===\")\n",
    "print(f\"7-day predictions:\")\n",
    "print(f\"  R² Score: {r2_7:.4f}\")\n",
    "print(f\"  MAE: {mae_7:.2f} MPa\")\n",
    "print(f\"  RMSE: {rmse_7:.2f} MPa\")\n",
    "print(f\"  Mean Uncertainty (±): {np.mean(y_pred_7_std):.2f} MPa\")\n",
    "print(f\"\\n28-day predictions:\")\n",
    "print(f\"  R² Score: {r2_28:.4f}\")\n",
    "print(f\"  MAE: {mae_28:.2f} MPa\")\n",
    "print(f\"  RMSE: {rmse_28:.2f} MPa\")\n",
    "print(f\"  Mean Uncertainty (±): {np.mean(y_pred_28_std):.2f} MPa\")\n",
    "\n",
    "# Calculate coverage of confidence intervals\n",
    "coverage_7 = np.mean((y_7_test_raw >= y_pred_7_ci_lower) & (y_7_test_raw <= y_pred_7_ci_upper))\n",
    "coverage_28 = np.mean((y_28_test_raw >= y_pred_28_ci_lower) & (y_28_test_raw <= y_pred_28_ci_upper))\n",
    "print(f\"\\n95% Confidence Interval Coverage:\")\n",
    "print(f\"  7-day: {coverage_7:.1%}\")\n",
    "print(f\"  28-day: {coverage_28:.1%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d831fa-a946-4bb6-8eb7-d8b4f6afd503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d9e93c-af5d-4ce9-9c3c-e1e0b92bc2b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 0. Data Preparation - CORRECTED: Split first, then scale\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Assume you have your original unscaled data: X, y_7, y_28\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# First, split the data BEFORE scaling\u001b[39;00m\n\u001b[1;32m     22\u001b[0m X_train_full, X_test, y7_train_full, y7_test, y28_train_full, y28_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mX\u001b[49m, y_7, y_28, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Now scale the features using StandardScaler\u001b[39;00m\n\u001b[1;32m     26\u001b[0m feature_scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import copy\n",
    "\n",
    "# =============================================================================\n",
    "# 0. Data Preparation - CORRECTED: Split first, then scale\n",
    "# =============================================================================\n",
    "# Assume you have your original unscaled data: X, y_7, y_28\n",
    "\n",
    "# First, split the data BEFORE scaling\n",
    "X_train_full, X_test, y7_train_full, y7_test, y28_train_full, y28_test = train_test_split(\n",
    "    X, y_7, y_28, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now scale the features using StandardScaler\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train_full)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the targets using MinMaxScaler (fitted only on training data)\n",
    "targets_train_df = pd.DataFrame({\n",
    "    \"7d CS\": y7_train_full,\n",
    "    \"28d CS\": y28_train_full\n",
    "})\n",
    "\n",
    "targets_test_df = pd.DataFrame({\n",
    "    \"7d CS\": y7_test,\n",
    "    \"28d CS\": y28_test\n",
    "})\n",
    "\n",
    "# Initialize and fit MinMaxScaler on training targets only\n",
    "target_scaler = MinMaxScaler()\n",
    "targets_train_scaled = target_scaler.fit_transform(targets_train_df)\n",
    "targets_test_scaled = target_scaler.transform(targets_test_df)\n",
    "\n",
    "# Split the scaled targets into separate arrays\n",
    "y7_train_scaled = targets_train_scaled[:, 0]\n",
    "y28_train_scaled = targets_train_scaled[:, 1]\n",
    "y7_test_scaled = targets_test_scaled[:, 0]\n",
    "y28_test_scaled = targets_test_scaled[:, 1]\n",
    "\n",
    "# Clean test data (same as training data since we split properly)\n",
    "X_test_clean = X_test_scaled\n",
    "y7_test_clean = y7_test_scaled\n",
    "y28_test_clean = y28_test_scaled\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Training: {len(X_train_scaled)}\")\n",
    "print(f\"Testing: {len(X_test_clean)}\")\n",
    "\n",
    "# Optional: Visualize the scaled target distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(y7_train_scaled, kde=True, alpha=0.7, label='Train')\n",
    "sns.histplot(y7_test_scaled, kde=True, alpha=0.7, label='Test')\n",
    "plt.title(\"Scaled 7-day CS\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y28_train_scaled, kde=True, alpha=0.7, label='Train')\n",
    "sns.histplot(y28_test_scaled, kde=True, alpha=0.7, label='Test')\n",
    "plt.title(\"Scaled 28-day CS\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. AGP Model Definition\n",
    "# =============================================================================\n",
    "class ConcreteDatasetScaled(Dataset):\n",
    "    def __init__(self, features, y_7_scaled, y_28_scaled):\n",
    "        self.z = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(np.vstack((y_7_scaled, y_28_scaled)).T, dtype=torch.float32)\n",
    "        self.t = torch.tensor([7.0, 28.0], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.t, self.y[idx]\n",
    "\n",
    "class AGPModelGP_v2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout_rate=0.1):\n",
    "        super(AGPModelGP_v2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dims[2])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], 5)\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc4(x)\n",
    "        \n",
    "        theta1 = self.softplus(out[:, 0])\n",
    "        theta2 = out[:, 1]\n",
    "        l = torch.exp(out[:, 2])\n",
    "        sigma_f = torch.exp(out[:, 3])\n",
    "        sigma_n = torch.exp(out[:, 4])\n",
    "        \n",
    "        return theta1, theta2, l, sigma_f, sigma_n\n",
    "\n",
    "def gp_neg_log_likelihood(theta1, theta2, l, sigma_f, sigma_n, t, y, epsilon=1e-6):\n",
    "    batch_size = y.shape[0]\n",
    "    n_t = t.shape[0]\n",
    "    total_nll = 0.0\n",
    "    log_t = torch.log(t + epsilon)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        m = theta1[i] * log_t + theta2[i]\n",
    "        diff = t.unsqueeze(0) - t.unsqueeze(1)\n",
    "        K = sigma_f[i]**2 * torch.exp(-0.5 * (diff**2) / (l[i]**2))\n",
    "        K = K + sigma_n[i]**2 * torch.eye(n_t, device=K.device)\n",
    "        K = K + epsilon * torch.eye(n_t, device=K.device)\n",
    "        \n",
    "        L_mat = torch.linalg.cholesky(K)\n",
    "        diff_y = (y[i] - m).unsqueeze(1)\n",
    "        alpha = torch.cholesky_solve(diff_y, L_mat)\n",
    "        log_det_K = 2.0 * torch.sum(torch.log(torch.diag(L_mat)))\n",
    "        nll = 0.5 * diff_y.t() @ alpha + 0.5 * log_det_K + 0.5 * n_t * np.log(2 * np.pi)\n",
    "        total_nll += nll.squeeze()\n",
    "    \n",
    "    return total_nll / batch_size\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Train AGP Model\n",
    "# =============================================================================\n",
    "# Create datasets\n",
    "train_dataset_full = ConcreteDatasetScaled(X_train_scaled, y7_train_scaled, y28_train_scaled)\n",
    "test_dataset_clean = ConcreteDatasetScaled(X_test_clean, y7_test_clean, y28_test_clean)\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=len(train_dataset_full))\n",
    "test_loader_clean = DataLoader(test_dataset_clean, batch_size=len(test_dataset_clean))\n",
    "\n",
    "# Initialize model with best parameters\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "best_dropout = 0.05\n",
    "best_lr = 0.002\n",
    "best_hidden_dims = [128, 128, 64]\n",
    "\n",
    "model_agp = AGPModelGP_v2(input_dim, hidden_dims=best_hidden_dims, dropout_rate=best_dropout).to(device)\n",
    "optimizer_agp = optim.Adam(model_agp.parameters(), lr=best_lr)\n",
    "t_fixed = torch.tensor([7.0, 28.0], dtype=torch.float32)\n",
    "\n",
    "# Training function\n",
    "def train_agp_model(model, train_loader, t, optimizer, num_epochs=300):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for z, _, y in train_loader:\n",
    "            z, y = z.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            theta1, theta2, l, sigma_f, sigma_n = model(z)\n",
    "            loss = gp_neg_log_likelihood(theta1, theta2, l, sigma_f, sigma_n, t.to(device), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training NLL Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"\\n--- Training AGP Model ---\")\n",
    "model_agp = train_agp_model(model_agp, train_loader_full, t_fixed, optimizer_agp, num_epochs=200)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MC Dropout Inference\n",
    "# =============================================================================\n",
    "def mc_predict_with_uncertainty(model, dataloader, t, num_samples=50, epsilon=1e-6):\n",
    "    model.train()  # Enable dropout during inference\n",
    "    all_mc_means = []\n",
    "    all_kernel_vars = []\n",
    "    \n",
    "    for z, _, y in dataloader:\n",
    "        z = z.to(device)\n",
    "        t_fixed_in = t.to(device)\n",
    "        mc_sample_means = []\n",
    "        mc_sample_vars = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            theta1, theta2, l, sigma_f, sigma_n = model(z)\n",
    "            m = theta1.unsqueeze(1) * torch.log(t_fixed_in + epsilon).unsqueeze(0) + theta2.unsqueeze(1)\n",
    "            mc_sample_means.append(m.detach().cpu().numpy())\n",
    "            \n",
    "            batch_kernel_vars = []\n",
    "            for i in range(z.size(0)):\n",
    "                diff = t_fixed_in.unsqueeze(0) - t_fixed_in.unsqueeze(1)\n",
    "                K = sigma_f[i]**2 * torch.exp(-0.5 * (diff**2) / (l[i]**2))\n",
    "                K = K + sigma_n[i]**2 * torch.eye(t_fixed_in.size(0), device=t_fixed_in.device)\n",
    "                K = K + epsilon * torch.eye(t_fixed_in.size(0), device=t_fixed_in.device)\n",
    "                K_inv = torch.linalg.inv(K)\n",
    "                \n",
    "                var_i = []\n",
    "                for j in range(t_fixed_in.size(0)):\n",
    "                    k_j = K[:, j].unsqueeze(1)\n",
    "                    var_j = K[j, j] - (k_j.t() @ K_inv @ k_j).item()\n",
    "                    var_i.append(float(max(var_j, 0)))\n",
    "                batch_kernel_vars.append(var_i)\n",
    "            mc_sample_vars.append(np.array(batch_kernel_vars))\n",
    "        \n",
    "        mc_sample_means = np.array(mc_sample_means)\n",
    "        mc_sample_vars = np.array(mc_sample_vars)\n",
    "        avg_kernel_vars = np.mean(mc_sample_vars, axis=0)\n",
    "        all_mc_means.append(mc_sample_means)\n",
    "        all_kernel_vars.append(avg_kernel_vars)\n",
    "    \n",
    "    mc_means = np.concatenate(all_mc_means, axis=1)\n",
    "    mc_vars = np.concatenate(all_kernel_vars, axis=0)\n",
    "    pred_mean = np.mean(mc_means, axis=0)\n",
    "    pred_epistemic_var = np.var(mc_means, axis=0)\n",
    "    pred_total_var = pred_epistemic_var + mc_vars\n",
    "    \n",
    "    return pred_mean, pred_total_var\n",
    "\n",
    "# Perform predictions\n",
    "print(\"\\n--- MC Dropout Inference on Test Set ---\")\n",
    "mc_mean, mc_var = mc_predict_with_uncertainty(model_agp, test_loader_clean, t_fixed, num_samples=50)\n",
    "\n",
    "# Convert to original scale\n",
    "y_pred_original = target_scaler.inverse_transform(mc_mean)\n",
    "\n",
    "# For MinMaxScaler, the variance scaling needs to be adjusted\n",
    "# MinMaxScaler scales to [0,1] range, so variance scales by (max-min)^2\n",
    "scale_range = target_scaler.data_range_\n",
    "var_7_original = mc_var[:, 0] * (scale_range[0]**2)\n",
    "var_28_original = mc_var[:, 1] * (scale_range[1]**2)\n",
    "std_7_original = np.sqrt(var_7_original)\n",
    "std_28_original = np.sqrt(var_28_original)\n",
    "\n",
    "# Ground truth - convert test targets back to original scale\n",
    "y_test_clean_combined = np.column_stack([y7_test_clean, y28_test_clean])\n",
    "y_true_original = target_scaler.inverse_transform(y_test_clean_combined)\n",
    "\n",
    "# Calculate R²\n",
    "r2_7_agp = r2_score(y_true_original[:, 0], y_pred_original[:, 0])\n",
    "r2_28_agp = r2_score(y_true_original[:, 1], y_pred_original[:, 1])\n",
    "\n",
    "print(f\"\\nAGP Test R² for 7-day: {r2_7_agp:.4f}\")\n",
    "print(f\"AGP Test R² for 28-day: {r2_28_agp:.4f}\")\n",
    "\n",
    "# Additional visualization of results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 7-day predictions\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_true_original[:, 0], y_pred_original[:, 0], alpha=0.6)\n",
    "plt.plot([y_true_original[:, 0].min(), y_true_original[:, 0].max()], \n",
    "         [y_true_original[:, 0].min(), y_true_original[:, 0].max()], 'r--', lw=2)\n",
    "plt.xlabel('True 7-day CS')\n",
    "plt.ylabel('Predicted 7-day CS')\n",
    "plt.title(f'7-day CS: R² = {r2_7_agp:.4f}')\n",
    "\n",
    "# 28-day predictions\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_true_original[:, 1], y_pred_original[:, 1], alpha=0.6)\n",
    "plt.plot([y_true_original[:, 1].min(), y_true_original[:, 1].max()], \n",
    "         [y_true_original[:, 1].min(), y_true_original[:, 1].max()], 'r--', lw=2)\n",
    "plt.xlabel('True 28-day CS')\n",
    "plt.ylabel('Predicted 28-day CS')\n",
    "plt.title(f'28-day CS: R² = {r2_28_agp:.4f}')\n",
    "\n",
    "# Uncertainty visualization\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(range(len(std_7_original)), std_7_original, alpha=0.6, label='7-day std', s=20)\n",
    "plt.scatter(range(len(std_28_original)), std_28_original, alpha=0.6, label='28-day std', s=20)\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Prediction Standard Deviation')\n",
    "plt.title('Prediction Uncertainty')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMean prediction uncertainty:\")\n",
    "print(f\"7-day CS: {np.mean(std_7_original):.4f} ± {np.std(std_7_original):.4f}\")\n",
    "print(f\"28-day CS: {np.mean(std_28_original):.4f} ± {np.std(std_28_original):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17debb7-1d8c-4092-b80d-c4db714f7df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /root\n",
      "Missing values in target columns:\n",
      "7d CS     621\n",
      "28d CS    667\n",
      "dtype: int64\n",
      "Dataset shape after dropping rows with missing targets: (603, 73)\n",
      "X shape: (603, 22)\n",
      "y_7 shape: (603,)\n",
      "y_28 shape: (603,)\n",
      "\n",
      "Missing values in cleaned DataFrame:\n",
      "SiO2                       0\n",
      "Al2O3                      0\n",
      "Fe2O3                      0\n",
      "CaO                        0\n",
      "MgO                        0\n",
      "Na2O                       0\n",
      "K2O                        0\n",
      "SO3                        0\n",
      "TiO2                       0\n",
      "P2O5                       0\n",
      "SrO                        0\n",
      "Mn2O3                      0\n",
      "LOI                        0\n",
      "AL/B                       0\n",
      "SH/SS                      0\n",
      "Ms                         0\n",
      "Ag/B                       0\n",
      "W/B                        0\n",
      "Sp/B                       0\n",
      "Initial curing temp (C)    0\n",
      "Final curing temp (C)      0\n",
      "Concentration (M) NaOH     0\n",
      "7d CS                      0\n",
      "28d CS                     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2091/709531187.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[\"Na2O\"] = df_clean[\"Na2O\"].fillna(df_clean[\"Na2O\"].median())\n",
      "/tmp/ipykernel_2091/709531187.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[\"Concentration (M) NaOH\"] = df_clean[\"Concentration (M) NaOH\"].fillna(df_clean[\"Concentration (M) NaOH\"].median())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import copy\n",
    "\n",
    "# Check current working directory (optional)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(r'/root/clean AAC concrete dataset large ratio.csv')\n",
    "\n",
    "# Remove the row where the \"Ref.\" column equals \"[107]\"\n",
    "df = df[df[\"Ref.\"] != \"[107]\"]\n",
    "\n",
    "# Define original feature columns and target columns\n",
    "original_feature_cols = [\n",
    "    \"SiO2\", \"Al2O3\", \"Fe2O3\", \"CaO\", \"MgO\", \"Na2O\", \"K2O\", \"SO3\",\n",
    "    \"TiO2\", \"P2O5\", \"SrO\", \"Mn2O3\", \"MnO\", \"LOI\", \n",
    "    \"AL/B\", \"SH/SS\", \"Ms\", \"Ag/B\", \"W/B\", \"Sp/B\",\n",
    "    \"Initial curing time (day)\", \"Initial curing temp (C)\", \n",
    "    \"Initial curing rest time (day)\", \"Final curing temp (C)\", \n",
    "    \"Concentration (M) NaOH\"\n",
    "]\n",
    "\n",
    "target_cols = [\"7d CS\", \"28d CS\"]\n",
    "\n",
    "# Drop unwanted columns: \"MnO\", \"Initial curing time (day)\", \"Initial curing rest time (day)\"\n",
    "cols_to_drop = [\"MnO\", \"Initial curing time (day)\", \"Initial curing rest time (day)\"]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Update the feature columns list accordingly (remove dropped columns)\n",
    "feature_cols = [\n",
    "    \"SiO2\", \"Al2O3\", \"Fe2O3\", \"CaO\", \"MgO\", \"Na2O\", \"K2O\", \"SO3\",\n",
    "    \"TiO2\", \"P2O5\", \"SrO\", \"Mn2O3\", \"LOI\", \n",
    "    \"AL/B\", \"SH/SS\", \"Ms\", \"Ag/B\", \"W/B\", \"Sp/B\",\n",
    "    \"Initial curing temp (C)\", \"Final curing temp (C)\", \n",
    "    \"Concentration (M) NaOH\"\n",
    "]\n",
    "\n",
    "# Convert target columns to numeric (replacing empty strings with NaN)\n",
    "for col in target_cols:\n",
    "    df[col] = pd.to_numeric(df[col].replace(' ', np.nan), errors='coerce')\n",
    "\n",
    "# Print missing values in target columns\n",
    "print(\"Missing values in target columns:\")\n",
    "print(df[target_cols].isnull().sum())\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df_clean = df.dropna(subset=target_cols)\n",
    "print(f\"Dataset shape after dropping rows with missing targets: {df_clean.shape}\")\n",
    "\n",
    "# Fill missing values in specific feature columns using the median\n",
    "df_clean[\"Na2O\"] = df_clean[\"Na2O\"].fillna(df_clean[\"Na2O\"].median())\n",
    "df_clean[\"Concentration (M) NaOH\"] = df_clean[\"Concentration (M) NaOH\"].fillna(df_clean[\"Concentration (M) NaOH\"].median())\n",
    "\n",
    "# Extract feature and target arrays BEFORE scaling\n",
    "X = df_clean[feature_cols].values\n",
    "y_7 = df_clean[\"7d CS\"].values\n",
    "y_28 = df_clean[\"28d CS\"].values\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y_7 shape:\", y_7.shape)\n",
    "print(\"y_28 shape:\", y_28.shape)\n",
    "\n",
    "# Check missing values in the cleaned DataFrame\n",
    "print(\"\\nMissing values in cleaned DataFrame:\")\n",
    "print(df_clean[feature_cols + target_cols].isnull().sum())\n",
    "\n",
    "# Step 1: Detect device and split data into training and test sets only\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "X_train, X_test, y7_train, y7_test, y28_train, y28_test = train_test_split(\n",
    "    X, y_7, y_28,\n",
    "    test_size=0.20,    # 20% for testing\n",
    "    random_state=42    # for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y7_train shape: {y7_train.shape}, y7_test shape: {y7_test.shape}\")\n",
    "print(f\"y28_train shape: {y28_train.shape}, y28_test shape: {y28_test.shape}\")\n",
    "\n",
    "# Step 2: Fit StandardScaler on training features only\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "\n",
    "print(\"\\nFeature scaling on training data complete.\")\n",
    "print(\"  Scaled train mean (≈0):\", np.mean(X_train_scaled))\n",
    "print(\"  Scaled train std  (≈1):\", np.std(X_train_scaled))\n",
    "\n",
    "# Step 3: Create a max‐value scaler for targets\n",
    "def max_strength_scaler(y7, y28, max_strength=100.0):\n",
    "    \"\"\"\n",
    "    Scale 7d and 28d compressive strengths by a fixed maximum value.\n",
    "    Returns:\n",
    "      y7_scaled:  np.array, scaled 7d strengths\n",
    "      y28_scaled: np.array, scaled 28d strengths\n",
    "      scaler:     object with inverse_transform(y) -> unscaled y\n",
    "    \"\"\"\n",
    "    y7_scaled  = y7 / max_strength\n",
    "    y28_scaled = y28 / max_strength\n",
    "\n",
    "    class MaxScaler:\n",
    "        def __init__(self, max_val):\n",
    "            self.max_val = max_val\n",
    "        def inverse_transform(self, y):\n",
    "            # y can be 1d or 2d\n",
    "            return y * self.max_val\n",
    "\n",
    "    return y7_scaled, y28_scaled, MaxScaler(max_strength)\n",
    "\n",
    "# Example usage on training targets:\n",
    "y7_train_scaled, y28_train_scaled, target_scaler = max_strength_scaler(y7_train, y28_train)\n",
    "\n",
    "print(\"First 5 scaled y7 values:\", y7_train_scaled[:5])\n",
    "print(\"First 5 scaled y28 values:\", y28_train_scaled[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b207ae-d3ce-435e-805d-882e9ea5fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteDatasetScaled(Dataset):\n",
    "    def __init__(self, features, y_7_scaled, y_28_scaled):\n",
    "        self.z = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(np.vstack((y_7_scaled, y_28_scaled)).T, dtype=torch.float32)\n",
    "        self.t = torch.tensor([7.0, 28.0], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.t, self.y[idx]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AGPModelGP_v2(nn.Module):\n",
    "    \"\"\"\n",
    "    Amortised GP v2:\n",
    "    - Input: feature vector x of dimension `input_dim`\n",
    "    - Outputs: GP kernel hyperparameters (θ1, θ2, ℓ, σ_f, σ_n)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout_rate=0.1):\n",
    "        super(AGPModelGP_v2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])  \n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])  \n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dims[2])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], 5)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, input_dim)\n",
    "        returns:\n",
    "          theta1: (batch_size,) positive transform via Softplus\n",
    "          theta2: (batch_size,) unconstrained real\n",
    "          l:      (batch_size,) positive via exp\n",
    "          sigma_f:(batch_size,) positive via exp\n",
    "          sigma_n:(batch_size,) positive via exp\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)   \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)  \n",
    "        out = self.fc4(x)\n",
    "        theta1  = self.softplus(out[:, 0])\n",
    "        theta2  = out[:, 1]\n",
    "        l       = torch.exp(out[:, 2])\n",
    "        sigma_f = torch.exp(out[:, 3])\n",
    "        sigma_n = torch.exp(out[:, 4])\n",
    "        \n",
    "        return theta1, theta2, l, sigma_f, sigma_n\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def gp_neg_log_likelihood(theta1, theta2, l, sigma_f, sigma_n, t, y, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the average negative log‐likelihood of a GP with\n",
    "    mean m(t)=θ1·log(t+ε)+θ2 and RBF kernel k(t,t')=σ_f^2 exp(−(t−t')²/(2ℓ²)),\n",
    "    plus noise variance σ_n^2.\n",
    "    \n",
    "    Args:\n",
    "        theta1, theta2, l, sigma_f, sigma_n: tensors of shape (batch,)\n",
    "        t:             tensor of shape (n_t,)  — time points (e.g. [7.,28.])\n",
    "        y:             tensor of shape (batch, n_t) — observed targets\n",
    "        epsilon:       small jitter to stabilize log and Cholesky\n",
    "    Returns:\n",
    "        avg_nll:       scalar tensor, average NLL over the batch\n",
    "    \"\"\"\n",
    "    batch_size, n_t = y.shape\n",
    "    total_nll = 0.0\n",
    "    log_t = torch.log(t + epsilon)\n",
    "    \n",
    "    diff = t.unsqueeze(0) - t.unsqueeze(1)  \n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        m_i = theta1[i] * log_t + theta2[i]\n",
    "        \n",
    "        # Covariance matrix K_i: shape (n_t, n_t)\n",
    "        K = sigma_f[i]**2 * torch.exp(-0.5 * (diff**2) / (l[i]**2))\n",
    "        K = K + (sigma_n[i]**2 + epsilon) * torch.eye(n_t, device=K.device)\n",
    "        \n",
    "        # Cholesky factor L: K = L Lᵀ\n",
    "        L = torch.linalg.cholesky(K)\n",
    "        \n",
    "        # Compute α = K^{-1}(y_i − m_i)\n",
    "        resid = (y[i] - m_i).unsqueeze(1)            # (n_t,1)\n",
    "        alpha = torch.cholesky_solve(resid, L)      # (n_t,1)\n",
    "        \n",
    "        # Log-determinant of K: log|K| = 2 ∑ log diag(L)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        \n",
    "        # NLL for this sample\n",
    "        nll_i = 0.5 * (resid.t() @ alpha) + 0.5 * log_det + 0.5 * n_t * np.log(2 * np.pi)\n",
    "        total_nll += nll_i.squeeze()\n",
    "    \n",
    "    # Return mean over batch\n",
    "    return total_nll / batch_size\n",
    "\n",
    "def train_agp_model(model, train_loader, t, optimizer, num_epochs=300):\n",
    "    model.train() #dropout active\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        for z, _, y in train_loader:\n",
    "            z, y = z.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            theta1, theta2, l, sigma_f, sigma_n = model(z)\n",
    "            loss = gp_neg_log_likelihood(theta1, theta2, l, sigma_f, sigma_n, t.to(device), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        if epoch == 1 or epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs}, Training NLL Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model, loss_history\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def gp_posterior_mean_var(\n",
    "    theta1, theta2, l, sigma_f, sigma_n,\n",
    "    t_train, y_train, t_pred, epsilon=1e-6\n",
    "):\n",
    "    # t_train, t_pred: 1-D tensors of shape (T,)\n",
    "    # theta1, theta2, l, sigma_f, sigma_n: 0-D tensors (scalars)\n",
    "    log_t_train = torch.log(t_train + epsilon)    # (T,)\n",
    "    log_t_pred  = torch.log(t_pred + epsilon)     # (T,)\n",
    "\n",
    "    # Broadcast scalar * vector + scalar → vector\n",
    "    m_train = theta1 * log_t_train + theta2       # (T,)\n",
    "    m_pred  = theta1 * log_t_pred  + theta2       # (T,)\n",
    "\n",
    "    # Build training covariance\n",
    "    diff_tt = t_train.unsqueeze(0) - t_train.unsqueeze(1)  # (T,T)\n",
    "    K = sigma_f**2 * torch.exp(-0.5 * diff_tt**2 / l**2)\n",
    "    K += (sigma_n**2 + epsilon) * torch.eye(t_train.numel(), device=K.device)\n",
    "\n",
    "    # Cholesky solve for α = K⁻¹(y - m)\n",
    "    L = torch.linalg.cholesky(K)\n",
    "    diff_y = (y_train - m_train).unsqueeze(1)              # (T,1)\n",
    "    alpha  = torch.cholesky_solve(diff_y, L)              # (T,1)\n",
    "\n",
    "    # Build cross-covariance for predictions\n",
    "    diff_tp = t_train.unsqueeze(1) - t_pred.unsqueeze(0)   # (T,T')\n",
    "    k_star  = sigma_f**2 * torch.exp(-0.5 * diff_tp**2 / l**2)  # (T,T')\n",
    "\n",
    "    # Posterior mean: m_pred + k_*^T α\n",
    "    mean_pred = (m_pred + (k_star.t() @ alpha).squeeze(1))    # (T',)\n",
    "\n",
    "    # Posterior variance: σf² − diag(k_*^T K⁻¹ k_*) + σn²\n",
    "    v = torch.linalg.solve_triangular(L, k_star, upper=False) # (T,T')\n",
    "    var_f = sigma_f**2 - (v * v).sum(dim=0)                   # (T',)\n",
    "    var_y = torch.clamp(var_f + sigma_n**2, min=epsilon)     # (T',)\n",
    "\n",
    "    return mean_pred, var_y\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def mc_predict_with_uncertainty(\n",
    "    model, dataloader, times, num_samples=150, epsilon=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (all in scaled space):\n",
    "      pred_mean       : (N, T) array of predictive means\n",
    "      epistemic_var   : (N, T) array of epistemic variances (dropout + process)\n",
    "      aleatoric_var   : (N, T) array of aleatoric variances (noise only)\n",
    "      total_var       : (N, T) array of total variances\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()  # keep dropout active\n",
    "\n",
    "    all_batch_means    = []\n",
    "    all_batch_process  = []\n",
    "    all_batch_noise    = []\n",
    "\n",
    "    for features, _, _ in dataloader:\n",
    "        features = features.to(device)\n",
    "        t_tensor = times.to(device)\n",
    "        B, T = features.size(0), t_tensor.numel()\n",
    "\n",
    "        # buffers to collect S samples\n",
    "        sample_means   = np.zeros((num_samples, B, T))\n",
    "        sample_process = np.zeros((num_samples, B, T))\n",
    "        sample_noise   = np.zeros((num_samples, B, T))\n",
    "\n",
    "        for s in range(num_samples):\n",
    "            slope, intercept, length_scale, signal_var, noise_var = model(features)\n",
    "\n",
    "            # 1) predictive mean for this sample\n",
    "            log_t = torch.log(t_tensor + epsilon)\n",
    "            mean_pred = slope.unsqueeze(1) * log_t.unsqueeze(0) + intercept.unsqueeze(1)\n",
    "            sample_means[s] = mean_pred.detach().cpu().numpy()\n",
    "\n",
    "            # 2) posterior process variance + noise variance\n",
    "            diff = t_tensor.unsqueeze(0) - t_tensor.unsqueeze(1)  # (T,T)\n",
    "            for i in range(B):\n",
    "                # signal kernel\n",
    "                K_signal = signal_var[i]**2 * torch.exp(-0.5 * diff**2 / length_scale[i]**2)\n",
    "                # full covariance\n",
    "                K_full = K_signal + (noise_var[i]**2 + epsilon) * torch.eye(T, device=device)\n",
    "                K_inv  = torch.linalg.inv(K_full)\n",
    "                for j in range(T):\n",
    "                    k_col       = K_full[:, j].unsqueeze(1)\n",
    "                    proc_var_ij = K_full[j, j].item() - (k_col.t() @ K_inv @ k_col).item()\n",
    "                    sample_process[s, i, j] = max(proc_var_ij, 0.0)\n",
    "                    sample_noise[s, i, j]   = noise_var[i].item()**2\n",
    "\n",
    "        # aggregate over MC samples for this batch\n",
    "        batch_process = sample_process.mean(axis=0)  # (B, T)\n",
    "        batch_noise   = sample_noise.mean(axis=0)    # (B, T)\n",
    "        all_batch_means.append(sample_means)         # list of (S, B, T)\n",
    "        all_batch_process.append(batch_process)      # list of (B, T)\n",
    "        all_batch_noise.append(batch_noise)          # list of (B, T)\n",
    "\n",
    "    # concatenate all batches\n",
    "    mc_means     = np.concatenate(all_batch_means,   axis=1)  # (S, N, T)\n",
    "    process_var  = np.concatenate(all_batch_process, axis=0)  # (N, T)\n",
    "    noise_var    = np.concatenate(all_batch_noise,   axis=0)  # (N, T)\n",
    "\n",
    "    # 3) compute variances\n",
    "    dropout_var    = mc_means.var(axis=0, ddof=0)       # Var over MC means\n",
    "    epistemic_var  = dropout_var + process_var         # dropout + process\n",
    "    aleatoric_var  = noise_var                         # noise only\n",
    "    total_var      = epistemic_var + aleatoric_var\n",
    "\n",
    "    # 4) predictive mean\n",
    "    pred_mean = mc_means.mean(axis=0)                  # (N, T)\n",
    "\n",
    "    return pred_mean, epistemic_var, aleatoric_var, total_var\n",
    "\n",
    "# Main execution: create datasets, initialize and train model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create PyTorch datasets from scaled data\n",
    "train_dataset = ConcreteDatasetScaled(X_train_scaled, y7_train_scaled, y28_train_scaled)\n",
    "test_dataset  = ConcreteDatasetScaled(X_test_scaled,  y7_test_scaled,  y28_test_scaled)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size   = len(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim     = X_train_scaled.shape[1]\n",
    "hidden_dims   = [128, 128, 64]\n",
    "dropout_rate  = 0.05\n",
    "learning_rate = 0.002\n",
    "num_epochs    = 250\n",
    "\n",
    "# Model and optimizer\n",
    "model     = AGPModelGP_v2(input_dim, hidden_dims, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Time‐points tensor\n",
    "t = torch.tensor([7.0, 28.0], dtype=torch.float32).to(device)\n",
    "\n",
    "# Train\n",
    "model, loss_history = train_agp_model(model, train_loader, t, optimizer, num_epochs)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loss_history is the list of avg NLL per epoch returned from train_agp_model\n",
    "epochs = list(range(1, len(loss_history) + 1))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss_history, label='Training NLL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Negative Log-Likelihood')\n",
    "plt.title('Training Loss History')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "max_strength = target_scaler.max_val\n",
    "\n",
    "for split_name, loader in [ (\"Test\", test_loader)]:\n",
    "    # run MC‐dropout\n",
    "    mean_s, epi_v, alea_v, tot_v = mc_predict_with_uncertainty(\n",
    "        model, loader, t, num_samples=100\n",
    "    )\n",
    "\n",
    "    # inverse‐scale to MPa\n",
    "    y_pred     = mean_s * max_strength\n",
    "    epi_std    = np.sqrt(epi_v)  * max_strength\n",
    "    alea_std   = np.sqrt(alea_v) * max_strength\n",
    "    total_std  = np.sqrt(tot_v)  * max_strength\n",
    "\n",
    "    # gather true targets from loader\n",
    "    _, _, y_scaled = next(iter(loader))\n",
    "    y_true = y_scaled.numpy() * max_strength\n",
    "\n",
    "    # compute metrics on MPa\n",
    "    r2_7   = r2_score(y_true[:,0], y_pred[:,0])\n",
    "    rmse_7 = np.sqrt(mean_squared_error(y_true[:,0], y_pred[:,0]))\n",
    "    mae_7  = mean_absolute_error(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "    r2_28   = r2_score(y_true[:,1], y_pred[:,1])\n",
    "    rmse_28 = np.sqrt(mean_squared_error(y_true[:,1], y_pred[:,1]))\n",
    "    mae_28  = mean_absolute_error(y_true[:,1], y_pred[:,1])\n",
    "\n",
    "    # print metrics\n",
    "    print(f\"\\n--- {split_name} Set Metrics (MPa) ---\")\n",
    "    print(f\"7-day → R²: {r2_7:.4f}, RMSE: {rmse_7:.3f}, MAE: {mae_7:.3f}\")\n",
    "    print(f\"28-day → R²: {r2_28:.4f}, RMSE: {rmse_28:.3f}, MAE: {mae_28:.3f}\")\n",
    "\n",
    "    # average uncertainties\n",
    "    print(\"Avg 1σ uncertainties:\")\n",
    "    print(f\"  7d epistemic:  {epi_std[:,0].mean():.3f} MPa\")\n",
    "    print(f\"  7d aleatoric: {alea_std[:,0].mean():.3f} MPa\")\n",
    "    print(f\"  7d total:     {total_std[:,0].mean():.3f} MPa\")\n",
    "    print(f\" 28d epistemic:  {epi_std[:,1].mean():.3f} MPa\")\n",
    "    print(f\" 28d aleatoric: {alea_std[:,1].mean():.3f} MPa\")\n",
    "    print(f\" 28d total:     {total_std[:,1].mean():.3f} MPa\")\n",
    "\n",
    "    # plot true vs pred\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for idx, title in enumerate([\"7-day\",\"28-day\"]):\n",
    "        plt.subplot(1,2,idx+1)\n",
    "        plt.scatter(y_true[:,idx], y_pred[:,idx], alpha=0.4)\n",
    "        mn, mx = y_true[:,idx].min(), y_true[:,idx].max()\n",
    "        plt.plot([mn,mx],[mn,mx],\"r--\")\n",
    "        plt.xlabel(f\"True {title} CS\")\n",
    "        plt.ylabel(f\"Pred {title} CS\")\n",
    "        plt.title(f\"{split_name}: {title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # print first 6 samples\n",
    "    print(\"\\nFirst 6 samples:\")\n",
    "    for i in range(6):\n",
    "        print(f\"Idx {i}: True=[{y_true[i,0]:.1f},{y_true[i,1]:.1f}]  \"\n",
    "              f\"Pred=[{y_pred[i,0]:.1f},{y_pred[i,1]:.1f}]  \"\n",
    "              f\"σ_epi=[{epi_std[i,0]:.1f},{epi_std[i,1]:.1f}]  \"\n",
    "              f\"σ_alea=[{alea_std[i,0]:.1f},{alea_std[i,1]:.1f}]  \"\n",
    "              f\"σ_tot=[{total_std[i,0]:.1f},{total_std[i,1]:.1f}]\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
